% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NNnFF.R
\name{ReLU}
\alias{ReLU}
\title{Activation function of a rectified linear unit (RELU).}
\usage{
ReLU(z)
}
\arguments{
\item{z}{A (real) matrix or vector.}
}
\value{
In r is identical to z except for the negative elements of z.
          These are set to 0.
}
\description{
Activation function of a rectified linear unit (RELU).
}
\examples{
a<-rndParms(numberOfNNParms(c(2, 3, 1)))
a
ReLU(a)
}
\seealso{
Other Feedforward Neural Network: 
\code{\link{NN}()},
\code{\link{P2NN}()},
\code{\link{RsquareNN}()},
\code{\link{numberOfNNParms}()},
\code{\link{printNNweights}()},
\code{\link{rndParms}()}
}
\concept{Feedforward Neural Network}
